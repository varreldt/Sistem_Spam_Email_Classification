# -*- coding: utf-8 -*-
"""Spam_Email_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11GINEGNG-IzX977WOW7F1N4raIj3tos7

# Import Libraries
"""

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import numpy as np  # Untuk Komputasi Numerik
import pandas as pd  # Untuk Manipulasi Data
import re  # Untuk mengganti satu atau banyak match dengan sebuah string
import joblib

"""## Import Dataset"""

df = pd.read_csv('spam.csv', encoding='latin-1')
df.head()

"""## Exploratory Data Analysis (EDA)"""

df.dropna(how="any", inplace=True, axis=1)
df.columns = ['label', 'message']
df.head()

df.describe()

df.groupby('label').describe()

# convert label to a numerical variable
df['label_num'] = df.label.map({'ham': 0, 'spam': 1})
df.head()

df['email_len'] = df.message.apply(len)
df.head()

"""## Text Pre-processing"""


def lowercase(dataset):
    # Mengubah semua huruf menjadi huruf kecil
    return dataset.apply(lambda x: ''.join([w for w in x.lower()]))


df['clean_message'] = lowercase(df['message'])
df

# Data Cleansing Remove URL, Username, Character, Short Words


def preprocess_remove(to_remove, clean_message):
    r = re.findall(to_remove, clean_message)
    for i in r:
        clean_message = re.sub(i, '', clean_message)
    return clean_message


df['clean_message'] = np.vectorize(preprocess_remove)(
    'http[\w]*', df['clean_message'])
df['clean_message'] = np.vectorize(
    preprocess_remove)('@[\w]*', df['clean_message'])
df['clean_message'] = df['clean_message'].str.replace('[^a-zA-Z]', ' ')
df['clean_message'] = df['clean_message'].apply(
    lambda x: ' '.join([w for w in x.split() if len(w) > 3]))
df

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Tokenization


def tokenization(inputs):
    return word_tokenize(inputs)


df['clean_message'] = df['clean_message'].apply(tokenization)
print(df.clean_message)

# Stopwords Removal

stop_words = set(stopwords.words('english'))
stop_words.remove('not')


def stopwordsRemove(inputs):
    return [item for item in inputs if item not in stop_words]


df['clean_message'] = df['clean_message'].apply(stopwordsRemove)
print(df.clean_message)

# Lemmatization

lemmatizer = WordNetLemmatizer()


def lemmatization(inputs):
    return [lemmatizer.lemmatize(word=x, pos='v') for x in inputs]


df['clean_message'] = df['clean_message'].apply(lemmatization)
print(df.clean_message)

# Menyatukan token ke dalam kalimat
df['clean_message'] = df['clean_message'].str.join(' ')
print(df.clean_message)


"""## Spliting the dataset into the Training set and Test set"""


X_train, X_test, y_train, y_test = train_test_split(
    df['clean_message'], df['label_num'], test_size=0.3, random_state=32)

print('Train set:', X_train.shape,  y_train.shape)
print('Test set:', X_test.shape,  y_test.shape)

"""## Feature Extraction with TF-IDF"""


fitur_train = TfidfVectorizer().fit(X_train)
tfidf_train = fitur_train.transform(X_train).toarray()
tfidf_test = fitur_train.transform(X_test).toarray()

"""## Building Model"""


X_train = tfidf_train
X_test = tfidf_test

nb = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
nb.fit(X_train, y_train)

y_pred = nb.predict(X_test)

print("Nilai sebenarnya : ", y_test)
print("Nilai prediksi : ", y_pred)

"""## Evaluation Model"""


print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Saving model to disk
joblib.dump(fitur_train, 'tf_idf.save')
joblib.dump(nb, 'nb.pkl')
