# -*- coding: utf-8 -*-
"""Spam_Email_Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11GINEGNG-IzX977WOW7F1N4raIj3tos7

## Import Libraries
"""

import numpy as np # Untuk Komputasi Numerik
import pandas as pd # Untuk Manipulasi Data
import re # Untuk mengganti satu atau banyak match dengan sebuah string

"""## Import Dataset"""
df = pd.read_csv('dataset/spam_en.csv', encoding='latin-1')
df.head()

"""## Exploratory Data Analysis (EDA)"""

df.dropna(how="any", inplace=True, axis=1)
df.columns = ['label', 'message']
df.head()

df.describe()

df.groupby('label').describe()

# convert label to a numerical variable
df['label_num'] = df.label.map({'ham':0, 'spam':1})
df.head()

df['email_len'] = df.message.apply(len)
df.head()

"""## Text Pre-processing"""

def lowercase(dataset):
  return dataset.apply(lambda x: ''.join([w for w in x.lower()])) # Mengubah semua huruf menjadi huruf kecil

df['clean_message'] = lowercase(df['message'])
df

# Data Cleansing Remove URL, Username, Character, Short Words
def preprocess_remove(to_remove, clean_message):
    r = re.findall(to_remove, clean_message)
    for i in r:
        clean_message = re.sub(i, '', clean_message)
    return clean_message

df['clean_message'] = np.vectorize(preprocess_remove)('http[\w]*',df['clean_message'])
df['clean_message'] = np.vectorize(preprocess_remove)('@[\w]*',df['clean_message'])
df['clean_message'] = df['clean_message'].str.replace('[^a-zA-Z]', ' ')
df['clean_message'] = df['clean_message'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))
df

import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')

# Tokenization
from nltk.tokenize import word_tokenize

def tokenization(inputs):
    return word_tokenize(inputs)

df['clean_message'] = df['clean_message'].apply(tokenization)
print(df.clean_message)

# Stopwords Removal
from nltk.corpus import stopwords

stop_words = set(stopwords.words('english'))
stop_words.remove('not')

def stopwordsRemove(inputs):
    return [item for item in inputs if item not in stop_words]

df['clean_message'] = df['clean_message'].apply(stopwordsRemove)
print(df.clean_message)

# Lemmatization
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

def lemmatization(inputs):
    return [lemmatizer.lemmatize(word=x, pos='v') for x in inputs]

df['clean_message'] = df['clean_message'].apply(lemmatization)
print(df.clean_message)

# Menyatukan token ke dalam kalimat
df['clean_message'] = df['clean_message'].str.join(' ')
print(df.clean_message)

"""## Spliting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df['clean_message'], df['label_num'], test_size=0.3, random_state=32)

print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

"""## Feature Extraction with TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

fitur_train = TfidfVectorizer().fit(X_train)
tfidf_train = fitur_train.transform(X_train).toarray()
tfidf_test = fitur_train.transform(X_test).toarray()

"""## Building Model"""

from sklearn.naive_bayes import MultinomialNB

X_train = tfidf_train
X_test = tfidf_test

nb = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
nb.fit(X_train, y_train)

y_pred = nb.predict(X_test)

print("Nilai sebenarnya : ", y_test)
print("Nilai prediksi : ", y_pred)

"""## Evaluation Model"""

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""## Save Model"""

import joblib

joblib.dump(fitur_train, 'tf_idf_en.save')
joblib.dump(nb, 'nb_en.pkl')

"""# Dataset Indonesia"""

df_id = pd.read_csv('dataset/spam_id.csv', encoding='latin-1')
df_id.head()

"""## EDA"""

df_id.describe()

df_id.groupby('label').describe()

df_id['email_len'] = df_id.Teks.apply(len)
df_id.head()

"""## Text Pre-propeces"""

def lowercase(dataset):
  return dataset.apply(lambda x: ''.join([w for w in x.lower()])) # Mengubah semua huruf menjadi huruf kecil

df_id['clean_message'] = lowercase(df_id['Teks'])
df_id

# Data Cleansing Remove URL, Username, Character, Short Words
def preprocess_remove(to_remove, clean_message):
    r = re.findall(to_remove, clean_message)
    for i in r:
        clean_message = re.sub(i, '', clean_message)
    return clean_message

df_id['clean_message'] = np.vectorize(preprocess_remove)('http[\w]*',df_id['clean_message'])
df_id['clean_message'] = np.vectorize(preprocess_remove)('@[\w]*',df_id['clean_message'])
df_id['clean_message'] = df_id['clean_message'].str.replace('[^a-zA-Z]', ' ')
df_id['clean_message'] = df_id['clean_message'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))
df_id

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

factory = StemmerFactory()
stemmer = factory.create_stemmer()

def stem_sentences(sentence):
    tokens = sentence.split()
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    return ' '.join(stemmed_tokens)

df_id['clean_message'] = df_id['clean_message'].apply(stem_sentences)
df_id

stop_words = stopwords.words('indonesian')

new_email_message = []
word_dic = []
for text in df_id['clean_message']:
    new_text = []
    for word in text.split():
        if word.lower() not in stop_words:
            new_text.append(word.lower())
            word_dic.append(word.lower())
    new_text = " ".join(new_text)
    new_email_message.append(new_text)
    
df_id['clean_message'] = new_email_message
df_id

"""## Spliting the dataset into the Training set and Test set"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_id['clean_message'], df_id['label'], test_size=0.3, random_state=32)

print ('Train set:', X_train.shape,  y_train.shape)
print ('Test set:', X_test.shape,  y_test.shape)

"""## Feature Extraction with TF-IDF"""

from sklearn.feature_extraction.text import TfidfVectorizer

fitur_train = TfidfVectorizer().fit(X_train)
tfidf_train = fitur_train.transform(X_train).toarray()
tfidf_test = fitur_train.transform(X_test).toarray()

"""## Building Model"""

from sklearn.naive_bayes import MultinomialNB

X_train = tfidf_train
X_test = tfidf_test

nb = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
nb.fit(X_train, y_train)

y_pred = nb.predict(X_test)

print("Nilai sebenarnya : ", y_test)
print("Nilai prediksi : ", y_pred)

"""## Evaluation Model"""

from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

print(accuracy_score(y_test, y_pred))
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""## Save Model"""

import joblib

joblib.dump(fitur_train, 'tf_idf_id.save')
joblib.dump(nb, 'nb_id.pkl')